{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Projects \n",
    "\n",
    "This document is there to help you sturcture your data science project. The flow of this guide will help you craft a data science project quickly and pay attention to some of the key elements of problem solving.\n",
    "\n",
    "When you are working on a data science project, these are some of the key parts that you need to work on: \n",
    "\n",
    "- Problem Statement\n",
    "- Data Acquisition\n",
    "- Data Dictionary\n",
    "- Feature extraction\n",
    "- Data Cleaning  \n",
    "- EDA and Data Visualization\n",
    "- Deriving Key insights from EDA\n",
    "- Model building\n",
    "- Evaluation \n",
    "- Deriving Key Insights from model\n",
    "- Exporting the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Problem statement\n",
    "\n",
    "You need clearly define the problem that you are solving. Typically when you are working with simple datasets like the UCI machine learning repository dataset then your problem statement is decided by your dataset. However the entire process of problem defintion involves translating business goals to analysis goals. \n",
    "\n",
    "Ideally you should choose a specific domain, choose a problem to solve then find data to solve the problem. In most cases exact data may not be available, then you can look at various data sources and combine data to get the required dataset. In practice however, you may have decided on what problem to solve on the basis of the what data is available and accessible to you. So you may have to go in the opposite direction where you need to decide on a dataset and then decide what questions I can ask of it and then attack the problem. \n",
    "\n",
    "Either way keep in mind, that in the process you need to learn about different domains.\n",
    "\n",
    "**--provide examples here--**\n",
    "\n",
    "- [ ] Clearly state your data source.\n",
    "- [ ] What type of data do you have? \n",
    "    - Structured/Unstructured.\n",
    "- [ ] What are you predicting?\n",
    "- [ ] What are your features? \n",
    "- [ ] What is your target? \n",
    "- [ ] What type of problem is it? \n",
    "    - [ ] Supervised/Unsupervised? \n",
    "    - [ ] Classification/Regression? \n",
    "- [ ] If you have to combine features to define the target, discuss that here.\n",
    "- [ ] Do you need to combine multiple data sources? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Acqusition\n",
    "\n",
    "In this section you will talk about how you found the data. \n",
    "\n",
    "- [ ] Make sure that you mention the source of your data.\n",
    "- [ ] If you are using multiple datasets, make sure you mention the source of all of those datasets \n",
    "and mention why you are using the datsets \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 3: Data Dictionary \n",
    "In this section you will be generating a data dictionary for your dataset. Data dictionaries are extremely important since they give us a quick glance at the various types of data we have. \n",
    "\n",
    "Typically each column of your dataset will have a data type associate with it. The data types that you would be typically using are: \n",
    "\n",
    "- Int\n",
    "- Float\n",
    "- String\n",
    "- Categorical\n",
    "- Datetime \n",
    "\n",
    "Try to cast each variable into one of these data types. The data dictionary would be a table with: \n",
    "- One column as a the feature/target name\n",
    "- One column for the data type\n",
    "- One column with a short description of the feature/target\n",
    "\n",
    "You can generate markdown tables at :\n",
    "https://www.tablesgenerator.com/markdown_tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Extraction\n",
    "\n",
    "- If you have unstructured data then in this step you need to extract features from the data to generate a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data cleaning\n",
    "\n",
    "Some points to keep in mind: \n",
    "\n",
    "- [ ] Find missing values. \n",
    "- [ ] Find NaN and 0 values. \n",
    "- [ ] Do all columns have the same dtypes?\n",
    "- [ ] Convert dates to datetime types.\n",
    "    - [ ] You can use the python package arrow or datetime.\n",
    "- [ ] Convert categorical variables to type 'category' if working with pandas. \n",
    "- [ ] Convert strings to ints or floats if they represent numbers.\n",
    "- [ ] Standardize strings\n",
    "    - [ ] Convert them to lower case if possible.\n",
    "    - [ ] Replace spaces with underscores or dashes.\n",
    "    - [ ] Remove white spaces around the string **this is very critical**.\n",
    "    - [ ] Check of inconsistent spellings *typically done manually*.\n",
    "- [ ] Look for duplicate rows or columns.\n",
    "- [ ] Look for preprocessed columns; example: A categorical column that has been duplicated \n",
    "    with categorical labels.\n",
    "    \n",
    "A list of data cleaning libraries: https://mode.com/blog/python-data-cleaning-libraries/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6:  Data preperation\n",
    "\n",
    "- [ ] Convert categorical features to dummy indices if you are doing regression or assign numerical labels if you are doing classification\n",
    "- [ ] Do test train split to generate a test set. Further do a train validation split, you will need to run the test train split function from sklearn twice for this purpose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Exploratory Data Analysis and Data Visualization\n",
    "\n",
    "There are multiple steps that you need to take here: \n",
    "\n",
    "- [ ] Identify outliers in the datsets. Keep track of them, we want to run to train the model with the outliers and without them to see their effect.\n",
    "- [ ] Check for imbalance in the target variable. Quantify the imbalance. \n",
    "- [ ] Pairplot if possible to check the relationship between all the features and the target.\n",
    "- [ ] Look at the histogram for each variable, try to identify if you have a symmetric or normal distribution.\n",
    "- [ ] If possible plot a QQ plot to check the normality of the data. If you want more information, refer to [this](https://refactored.ai/learn/normality-tests/24c311b1936a4037b29ef78d629f1320/).\n",
    "- [ ] If its a classification problem, run a chi-square test between each categorical feature and the target to check for correlation and run ANOVE between the continuous/discrete features and the target to check for correlations.\n",
    "- [ ] If its a regression problem get pearson correlations between the continuous features and target and run ANOVA between each categorical variable and target.\n",
    "- Check for correlations between individual features; use similar approaches as you did with the target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Key Insights from EDA\n",
    "In this section you will present what you have learnt from performing Exploratory Data Analysis. At the end of this section you should have written down the following:\n",
    "\n",
    "- [ ] A bullet point list of relationship between features and target and between individual features.\n",
    "- [ ] A written summary of what the conclusions of the exploratory data analysis.\n",
    "\n",
    "The first point involves writing down what type of correlations observed between each feature and target. For regression you can state the correlation value ( r-squared value) between the feature and the target for classification you can state the p-value of the chi-square test with the conclusion of weather you are accepting or reject the null hypothsis. The same should be done for between individual features. \n",
    "\n",
    "The second part involves writing a small summary of part 1. You should mention in words the conclusion that you reached. \n",
    "\n",
    "There may be situations where you maybe compelled to drop a varible because you have either too many outliers or it has strong correlations with the output. You may even want to create new variables using external data that you have imported. You should document and discuess these changes in this section. \n",
    "\n",
    "\n",
    "### Data visualizations\n",
    "\n",
    "For each pair of variables (i.e feature vs feature and feature vs target) \n",
    "you need to have a seperate section with visualizations. Based on the type of target and feature you may need to choose an approrpriate plot (Histogram, time-series plots, scatter plots etc)\n",
    "\n",
    "Make sure that you label the plots properly: \n",
    "Each plot should have the following: \n",
    "- [ ] Readable and descriptive axis labels.\n",
    "- [ ] Font size of atleast 12 for the x-y axis tick labels.\n",
    "- [ ] A title.\n",
    "- [ ] A legend that label a curve. Even if you have a single curve.\n",
    "- [ ] If you are using a scatter plot, no fancy points, just use circles, triangles and other simple geometric shapes.\n",
    "- [ ] Make sure that everything is easy to read plot. Use colors meanigfully; most plots do not need several colors. \n",
    "- [ ] Try to make sure that you start at the origin (x=0, y=0). Be aware of the scale of your data. \n",
    "\n",
    "\n",
    " Make sure you save your final visualizations in the ```/images/``` folder and call them from the images folder. Make sure you given the images meaningful names. For example you can name an EDA image between two features as \"eda_scatter_feature1_feature2.png\". Everyone has their own naming convention. Make sure that you stick to that convention. \n",
    " \n",
    "### Importing Images into a Jupyter notebook \n",
    " There may be times where you might want to import images into jupyter notebooks. These might be other plots or plots you have generated and saved. \n",
    " \n",
    " Images can be imported to a jupyter notebook from a different directory using a relative path. \n",
    "A realtive path example is: \n",
    "\n",
    "```../../../images/image1.png```\n",
    "\n",
    "This means that we are going 3 folders up and then into the image folder and referencing ```image1.png```. \n",
    "In order to import the image we would write\n",
    "\n",
    "```<img src=\"../../../images/image1.png\">``` \n",
    "\n",
    "where we are using the ```img``` html tag to generate the image. This is written down in a markdown cell of the jupyter notebook. NOT a code cell. \n",
    "\n",
    "It is best practice to write down the above html tag in a seprate markdown cell. This just make it easier to access and troubleshoot. \n",
    "\n",
    "**Note: If you are a DS1 student then head straight to the summary section**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Model building\n",
    "\n",
    "This section is relevant to only DS2 and DS3 students. \n",
    "\n",
    "Here you will train a model on your training set. Make sure you train the model then get predictions on the training set. We want to keep track of this since we want to check for overfitting. We will compare the results from the training set to testing set to see if we are overfitting. \n",
    "\n",
    "Typically, if you have high training accuracy (or low r-square value for regression) and low testing set accuracy ( or high r-squared value for regression) it mean you are overfitting. In such a situation you need to go to a more complex model. Perhaps use regularization or get more data. \n",
    "\n",
    "\n",
    "For DS2 students. You would have been exposed to only- Linear regression, Polynomial regression, Logistic regression and Naive bayes. Hence you get to choose from these algorithms based on the type of problem, classification/regression, that you have chosen \n",
    "\n",
    "For DS3 students. You have learnt a whole bunch of algorithms and techniques hence this a place to try them out. Start with a simple baseline model and then try more complex models. Example for classification: \n",
    "- Start with baseline model as - Logistic regression or Naive Bayes\n",
    "- Then try Decision tree\n",
    "- Then try SVC \n",
    "- Then Random forests\n",
    "- Then XGBoost\n",
    "\n",
    "The same can be said for regression. Running multiple models and looking at the accuracy and confusion matrix will help you understand how to judge each model on the training set. \n",
    "\n",
    "If possible you want to utlize gridsearch to find parameters. Gridsearch is great for finding parameters especially when you have a many of them. \n",
    "https://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Model evaluation \n",
    "Once you manage to train your model. You must evalue its performance on the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Key Insights from Predictive analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Exporting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Project Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
